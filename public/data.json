{
   "title":"A database for 2022",
   "author":null,
   "date_published":null,
   "dek":null,
   "lead_image_url":"https://tailscale.com/files/images/og-image.png",
   "content":" Hi, it’s us again, the ones who used to\nstore our database in a single JSON file\non disk, and then moved to etcd.\nTime for another change!\nWe’re going to put everything in a single file on disk again.\nAs you might expect from our previous choice (and as many on the\ninternet already predicted), we ran into some limits with etcd.\nDatabase size, write transaction frequency, of particular note:\ngenerating indexes.\nAll of these were surmountable limits, but we were quickly running\ninto the biggest limit: me.\nUntil now, I have been allowed to choose just about anything\nfor a database as long as I do all the work.\nBut at a certain point, that doesn’t scale.\nThe way to solve the issues with etcd was bespoke code.\nEvery time someone else had to touch it, I had to explain it.\nEspecially the indexing code. (Sorry.)\nWhat we need is something easier to dive into and get back out of\nquickly, a database similar enough to common development systems that\nother engineers, working hard to solve other problems, don’t have to\nget distracted by database internals to solve their problem.\nReaching this team engineering scaling limit was entirely predictable,\nthough it happened faster than we thought it would.\nSo we needed something different, something more conservative than our\nprevious choices.\nThe obvious candidates were MySQL (or one of its renamed variants\ngiven who bought it) or PostgreSQL, but several of us on the team have\noperational experience running these databases and didn’t enjoy the\nprospect of wrestling with the ops overhead of making live replication\nwork and behave well.\nOther databases like CockroachDB looked very tempting, but we had zero\nexperience with it.\nAnd we didn’t want to lock ourselves into a cloud provider with a\nmanaged product like Spanner.\nWe have several requirements in our previous blog\npost that still apply, such as being able to run our entire test\nsuite locally and hermetically quickly and easily, ideally without\nVMs or containers.\nEnter Litestream\nThere is one very fun database out there: SQLite.\nBut live replication typically involves layers on top of SQLite, which\nintroduces a lot of the operational overhead risks of other databases,\nonly with systems that are less widely deployed and so not as well known.\nHowever, something new has appeared on the SQLite front that makes\nlive-replication feasible, without interpositing itself between your\napplication and SQLite: litestream.\nLitesteam is neat, because it’s conceptually so simple.\nIn WAL-mode (the mode you very much want on a server, as it means\nwriters do not block readers), SQLite appends to a WAL file and then\nperiodically folds its contents back into the main database file as\npart of a checkpoint.\nLitestream interposes itself in this process: it grabs a lock so that\nno other process can checkpoint.\nIt then watches the WAL file and streams the appended blocks up to S3,\nperiodically checkpointing the database for you when it has the\nnecessary segments uploaded.\nThis gives you near real time backups (or with a couple deft modifications, lets your app block at critical sections until the backup is done) and lets you replay your database from S3 trivially, using SQLite’s standard WAL reading code. No modifications to SQLite necessary. It’s a great hack, and a small enough program that I could read my way through it entirely before committing to it.\nMigration, step 0\nFirst off, we took this opportunity to move some low-value ephemeral data that had too many writes/sec for etcd to be happy into SQLite. We ended up creating a separate SQLite DB for it, because we didn’t want to stream every update of this low-value database to S3. This took longer than expected because I took the opportunity to make a set of schema changes to the data.\nThis wasn’t necessary at all to migrate etcd, but was one of the criteria we used to judge a database replacement: could it do more than etcd? SQLite did a good job of this. Two databases adds some complexity, but SQLite has good semantics for ATTACH that make it straightforward to use.\nEarlier I said we migrated to one file on disk but I guess that’s not quite accurate; we have two files on disk now: our “main” SQLite database for high-value data and our “noise” SQLite database for ephemeral data. Or four files if you count the WALs.\nMigration, step 1\nThe core of the migration could be done quickly. We defined an SQLite table to hold the key-value pairs from etcd:\nCREATE TABLE IF NOT EXISTS main.DBX (\n        Key   TEXT PRIMARY KEY, -- the etcd key\n        Value TEXT              -- JSON-encoded types from etcd\n);\nThen we did a three-stage deploy:\nWe modified our etcd client wrapper to start writing all KV-pairs into both etcd and SQLite.\nWe then changed our etcd wrapper to read from sqlite as the source of truth.\nThen we turned off writing to etcd.\nBy the end of it we were left with an etcd cluster we could turn off.\nMigration, step 2\nThe second step is slowly moving data out of that DBX table into custom tables per type. This is going slowly. We’ve done several tables. Each one requires extensive changes to our service code to do well, so each requires a lot of thought. SQLite doesn’t seem to be getting in the way of this process though.\nI did end up writing quite a lot of “schema migration” code for doing rollouts. I feel like more of this should have been available for SQL versioning off the shelf.\nExperience Report\nHow did it go? Good question. SQLite works as it says on the tin. It requires some tuning for working under load, we have another post coming about that. The migration from etcd to one-big-table in SQLite was easy.\nThe process of changing the schema, pulling data out of the generic\ntable, is somewhat painful.\nIt’s slow because I doesn’t have many hours for programming\nany more, and slow because changes have to be carefully rolled out.\nWe don’t think SQLite is the limiting factor here though, it’s the\nway our code uses the database.\nIn retrospect I could have designed more layers into the\noriginal control service to make this easy.\n(You can say similar things about a lot of the code we wrote in the\nearly days.)\nWe are slowly getting the DB code to a place where I do not\nfeel bad inflicting it on co-workers, which was the primary objective\nof this migration.\nWe won’t be truly happy until all the old etcd indexing code and\ncaching layer is gone, but we’re moving in the right direction.\nIt should be easy to work on our service.\nMost of the negative experiences are in our old retrofitted etcd caching layer, and we can’t blame SQLite for that. No databases have precisely the same semantics.\nOne interesting SQLite gotcha we ran into: our two databases are\nATTACHed.\nWhen a default write transaction starts with BEGIN DEFERRED, each\ndatabase has to be locked.\nThe order they are locked in is determined by which one is\nUPDATEd or INSERTed into first, which can cause a deadlock with\nthe single SQLite writer when two different transactions lock in a\ndifferent order.\nWe resolved this by always using BEGIN IMMEDIATE on write transactions.\nWe’re also looking forward to read-only litestream replicas, which we\nintend to set up in the future.\nFootnote: coworkers point out it’s April Fool’s today and request that\nI clarify this isn’t a joke. Joke’s on them: every day’s April Fool’s\nin the Tailscale Database Engineering department. ",
   "next_page_url":null,
   "url":"https://tailscale.com/blog/database-for-2022/",
   "domain":"tailscale.com",
   "excerpt":"Hi, it’s us again, the ones who used to store our database in a single JSON file on disk, and then moved to etcd. Time for another change! We’re going to put everything in a single file on disk again.",
   "word_count":1273,
   "direction":"ltr",
   "total_pages":1,
   "rendered_pages":1
}